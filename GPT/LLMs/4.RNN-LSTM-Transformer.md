# RNN과 LSTM: 시퀀스 데이터 처리의 시작
**순환 신경망(Recurrent Neural Network, RNN)**은 시퀀스 데이터를 처리하기 위해 고안된 신경망 구조로, 이전의 출력을 현재의 입력으로 사용하는 구조를 가지고 있습니다 .

그러나 RNN은 장기 의존성 문제로 인해 긴 시퀀스의 정보를 효과적으로 처리하기 어렵습니다. 이를 해결하기 위해 장단기 기억(Long Short-Term Memory, LSTM) 구조가 도입되었으며, 이는 정보를 장기간 보존하고 필요할 때만 업데이트하는 게이트 구조를 가지고 있습니다 .

- 정리 : RNN/LSTM는  시퀀스 데이터를 처리하기 위한 초기 딥러닝 구조.

# Transformer: 병렬 처리와 장기 의존성 해결
Transformer는 RNN의 한계를 극복하기 위해 제안된 모델로, **자기 주의 메커니즘(Self-Attention)**을 활용하여 입력 시퀀스의 모든 위치를 동시에 고려합니다. 이를 통해 **전체 문장 병렬 처리**가 가능하며, 장기 의존성 문제를 효과적으로 해결할 수 있습니다 .

## **자기 주의 메커니즘(Self-Attention)**
- 이는 한 문장 안에서 각 단어가 다른 단어들과 얼마나 관련 있는지를 스스로 계산해서 반영하는 방법이에요.
- 예시:
```plainText
문장: "그는 사과를 먹었다. 그것은 달콤했다."
여기서 "그것"이 가리키는 것이 "사과"인지, "그"인지 판단할 수 있어야 하죠.
Self-Attention은 이런 관계를 수치로 계산해서 반영합니다.
```

### 구조
Transformer는 두 부분으로 구성됩니다:
- 인코더(Encoder): 입력 문장을 받아 내부 표현으로 변환
  - 인코더 내부 구성
        - Multi-head Attention (자기 주의 여러 번)
        - Feed Forward Network
        - Layer Normalization
        - Residual Connection
        - 이 구조가 여러 층(Stack)으로 반복됩니다.
- 디코더(Decoder): 그 표현을 바탕으로 출력(예: 번역된 문장) 생성